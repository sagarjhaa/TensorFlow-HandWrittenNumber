{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST - just code - github.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagarjhaa/TensorFlow-HandWrittenNumber/blob/master/MNIST_just_code_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7_m7eDz7qd0s",
        "colab_type": "code",
        "outputId": "70d08434-db96-40c0-c18f-553260ceec6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras # a machine learning library that lets you create neural networks\n",
        "from keras.datasets import mnist # the needed dataset\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Dense, Dropout, Flatten, Activation # importing different types of layers that you can use with your model\n",
        "from keras.layers import Conv2D, MaxPooling2D, MaxPooling1D # same as above\n",
        "from keras import backend as K\n",
        "import random # lets us generate random numbers - used when looking at random entries from the dataset.\n",
        "import matplotlib # lets us display images\n",
        "import numpy as np # library for delaing with arrays of numbers - the data is represented as an array\n",
        "import tensorflow as tf # the backend machine learning library that keras relies on\n",
        "import matplotlib.pyplot as plt #for charts at end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "kcdyBcXOqgXG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data() # downloading the MNIST dataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cn8h3AS7vZvk",
        "colab_type": "code",
        "outputId": "fc26bd31-8c13-48ce-d18a-039eaec7e72d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        }
      },
      "cell_type": "code",
      "source": [
        "np_x_train = np.array(x_train[0]).reshape(28,28).tolist()\n",
        "\n",
        "for i in range(0,len(np_x_train)):\n",
        "  #print(np_x_train[i])\n",
        "  print(list(map(lambda x : \"{0:0=3d}\".format(x),np_x_train[i])))\n",
        "\n",
        "matplotlib.pyplot.matshow(x_train[0])\n",
        "# print(\"Size of the array : \" + str(x_train[0].shape))\n",
        "# matplotlib.pyplot.matshow(x_train[157])\n",
        "print(\"The ground truth for this entry is: \" + str((y_train[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '003', '018', '018', '018', '126', '136', '175', '026', '166', '255', '247', '127', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '030', '036', '094', '154', '170', '253', '253', '253', '253', '253', '225', '172', '253', '242', '195', '064', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '049', '238', '253', '253', '253', '253', '253', '253', '253', '253', '251', '093', '082', '082', '056', '039', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '018', '219', '253', '253', '253', '253', '253', '198', '182', '247', '241', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '080', '156', '107', '253', '253', '205', '011', '000', '043', '154', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '014', '001', '154', '253', '090', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '139', '253', '190', '002', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '011', '190', '253', '070', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '035', '241', '225', '160', '108', '001', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '081', '240', '253', '253', '119', '025', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '045', '186', '253', '253', '150', '027', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '016', '093', '252', '253', '187', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '249', '253', '249', '064', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '046', '130', '183', '253', '253', '207', '002', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '039', '148', '229', '253', '253', '253', '250', '182', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '024', '114', '221', '253', '253', '253', '253', '201', '078', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '023', '066', '213', '253', '253', '253', '253', '198', '081', '002', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '018', '171', '219', '253', '253', '253', '253', '195', '080', '009', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '055', '172', '226', '253', '253', '253', '253', '244', '133', '011', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '136', '253', '253', '253', '212', '135', '132', '016', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "['000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000', '000']\n",
            "The ground truth for this entry is: 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVMAAAFSCAYAAABPFzzRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGEKAAYIUwAw8D91xzGJtlqCrgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 396x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "IgQbGXLXqpF4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# just defining some parameters to train the network\n",
        "batch_size = 1#128\n",
        "num_classes = 10 # how many different \"classes\" of numbers there are\n",
        "epochs = 5 # how many times the network will look at the entire dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WdJQtI9PrJ9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_rows, img_cols = 28, 28 # the \"shape\" of the input data; we need to define that the images are 28x28, "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HcyAjBSvrsH_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if K.image_data_format() == 'channels_first': # code for flattening the images into 28x28x1 (black and white) instead of 28x28x3 (rgb full color images)\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KBCelxbGrzFS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'numbers the network will learn from (train on)')\n",
        "print(x_test.shape[0], 'numbers the network will not learn from (will only be used for testing)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r71-xdXXr3cz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# One hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cvBmxE0rr9Y1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# your neural network goes here:\n",
        "# ----------------------------- #\n",
        "\n",
        "\n",
        "model.add(Flatten(input_shape=input_shape))\n",
        "model.add(Dense(128, activation='tanh'))\n",
        "model.add(Dense(128, activation='sigmoid'))\n",
        "model.add(Dense(128, activation='sigmoid'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128))\n",
        "\n",
        "#model.add(Dense(units=64, activation='relu', input_dim=100))\n",
        "model.add(Dense(num_classes, activation=Activation(tf.nn.softmax)))\n",
        "\n",
        "\n",
        "# ----------------------------- #\n",
        "# end your neural network\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fWVY39H2r95u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y-GLTchgsApG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "validation_data=(x_test, y_test))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('step')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MFC6XuRnsDYK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}